---
title: "mlr_1"
author: "Daniel_Kim"
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: 
    html_notebook: 
      toc_float: true
      toc : true
      toc_depth : 5
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# install.packages("mlr3")
library(mlr3)
# install.packages('mlr3viz')
library(mlr3viz)
library(data.table)
library(tidyverse)
# install.packages('mlr3learners')
library(mlr3learners)
# install.packages('precrec')
library(precrec)
```

# 2 Basics

 - task
    * Tasks encapsulate the data with meta-information, such as the name of the prediction target column
    * access predefined tasks
    * specify a task type
    * create a task
    * work with a task's API
    * assign roles to rows and columns of a task
    * implement task mutators, and
    * retrieve the data that is stored in a task
  
 - Learners
    * Learners encapsulate machine learning algorithms to train models and make predictions for task
    * They are provided by R and other packages.
    * access the set of classification and regression learners that come with mlr3 and retrieve a specific learner
    * access the set of hyperparameter values of a learner and modifiy them
    
 - Train and predict
    * set up tasks and learners properly
    * set up train and test splits for a task
    * train the learner on the training set to produce a model
    * generate predictions on the test set, and
    * assess the performance of the model by comparing predicted and actual values
  
 - Resampling
    * access and select resampling strategies
    * instantiate the split into training and test sets by applying the resampling, and
    * excute the resampling to obtain results
    
 - Benchmarking
    * create a benchmarking design
    * execute a design and aggreate results, and
    * convert benchmarking object to resampling objects
    
  
 - Binary classification 
    * ROC curves and the threshold where to predict one class versus the other, and
    * threshold tuning(WIP)
    
    
## 2.1 Quick R6 Into for Beginners

 - R6 is one of R's more recent dialects for object-oriented programming.
 
 - Objects are created by calling the constructor of an `R6Class()` object, specifically the `$new()` method.
    * `foo = Foo$new(bar = 1)` creates a new object of class `Foo`, setting the `bar` argument of the constructor to 1.
    
 - Classes have mutable state, which is encapsulated in their field, which can be accessed through the dollar operator.
    * `foo$bar = 2` : to set the `bar` value of Foo class as 2
    
 - In additon to fields, objects expose mehtods that may allow to inspect the object's state, retrieve information, or perform an action that may change the internal state of the object. 
    * `$train` method of a learner changes the internal state of the learner by building and storing a trained model, which can then be used to make predictions given data
    
 - Objects can have public and private fields and methods. In mlr3, you can only access the public variables and methods. 
 
 - R6 variables are references to objects rather than the actual objects, which are stored in an environment.
    * `foo2 = foo` does not create a copy of foo.
    * `foo$bar = 3` will also change `foo$bar` to 3 and vice versa.
    
 - To copy an object, use the `$clone()` method and the `deep = TRUE` argument for nested objects
    * `foo2 = foo$clone(deep = TRUE)`
    
    
## 2.2 Tasks

Tasks are objects that contain the data and additional meta-data for a machine-learning problem.

### 2.2.1 Task Types

To create a task from a data.frame() or data.table() object, the task type needs to be specified:

- Classification Task: The target is a label (stored as character() or factor()) with only few distinct values,
   - [mlr3::TaskClassif](https://google.com)
   
- Regression Task: The target is a numeric quantity (stored as interger() or double())
    - mlr3::TaskRegr
    
- Survival Task: The target is the (right-censored) time to an event.
    - mlr3proba::TaskSurv in add-on package mlr3proba

- Ordinal Regression Task: The target is ordinal.
    - TaskOrdinal in add-on package mlr3ordinal
    
 - Cluster Task: An unsupervised task type; there is no target and the aim is to identify similar groups within the feature space.
    - Not yet implemented
    
 - Spatial Task: Observation in the task have spatio-temporal information (e.g. coordinates). 
    - Not yet implemented, but started in add-on package mlr3spatiotemporal
    
### 2.2.2 Task Creation

- mtcars regression example

- preparing dataset

```{r}
data('mtcars', packages = 'datasets')
data = mtcars[, 1:3]
str(data)
```

- create the task using the constructor for a regression task object (`TaskRegr$new`) 
1. `id`: identifier for the task
2. `backend`: to control over how data is accessed. `DataBackendDataTable`
3. `target`: The name of the target column for the regression problem


```{r}
task_mtcars = TaskRegr$new(id = 'cars', backend = data, target = 'mpg')
print(task_mtcars)
```


- `mlr3viz`

```{r}
autoplot(task_mtcars, type = 'pairs')
```


### 2.2.3 Predefined tasks

mlr3 ships with a few predefined machine learning tasks. All tasks are stored in an R6 Dictionary (a key-value store) named `mlr_tasks`.

```{r}
mlr_tasks
```

```{r}
as.data.table(mlr_tasks)
```


```{r}
task_iris = mlr_tasks$get('iris')
print(task_iris)
```


```{r}
tsk('iris')
```



### 2.2.4 Task API

#### 2.2.4.1 Retrieving Data

```{r}
task_iris$nrow
```


```{r}
task_iris$ncol
```

```{r}
task_iris$data()
```

row_ids as integer

```{r}
head(task_iris$row_ids)
```

`data(rows = c(...))`

```{r}
task_iris$data(rows = c(1, 51, 101))
```

target and feature columns also have unique identifiers, i.e. names. These names are stored in the public slots `$feature_names` and `$target_names`.

```{r}
task_iris$target_names
```

```{r}
task_iris$feature_names
```

```{r}
task_iris$data(rows = c(1, 51, 101), cols = 'Species')
```


To extract the complete data from task, one can simply convert it to a data.table()

```{r}
task_iris %>% as.data.table()
```

```{r}
task_iris %>% as.data.table() %>% summary()
```

#### 2.2.4.2 Roles(Rows and Columns)

```{r}
task_mtcars$col_roles
```

To add the row names of `mtcars` as an additional feature, we first add them to the data table and then recreate the task.

```{r}
# with `keep.rownames`,  data.table stores the row names in an extra column 'rn'

data = as.data.table(mtcars[, 1:3], keep.rownames = TRUE)
data
```

```{r}
task_mtcars_1 = TaskRegr$new(id = 'cars', backend = data, target = 'mpg')
```


```{r}
task_mtcars_1$feature_names
```

```{r}
task_mtcars_1 %>% as.data.table()
```


We will change the role of the "rn" column and remove it from the set of active features. This is done by simply modifying the field `$col_roles`

```{r}
# suppported column roles, see ?Task
names(task_mtcars_1$col_roles)
```

```{r}
# assign column 'rn' the role 'name'
task_mtcars_1$col_roles$name = 'rn' # use chain /s self in R6 class is chainable.

task_mtcars_1$col_roles
```

```{r}
# remove 'rn' from role 'feature'
task_mtcars_1$col_roles$feature = setdiff(task_mtcars_1$col_roles$feature, 'rn')
```

```{r}
task_mtcars_1$feature_names
```

'rn' does not appear anymore when we access the data

```{r}
task_mtcars_1$data(rows = 1:5)
```


Changing the role does not change the underlying data. Changing the role only change the view on it.

Row can have two different roles:

1. Role `use`: Rows that are generally available for model fitting (although they may akso be used as test set in resampling). This role is the default role.

2. Role `validation`: Rows that are not used for training. Rows that have missing values iin the target column during task creation are automatically set to the validation role.

#### 2.2.4.3 Task Mutators

`$filter()` subsets the current view based on row ids and` $select()` subsets the view on the data.

```{r}
task = tsk('iris')
task$select(c('Sepal.Width', 'Sepal.Length'))
task$filter(1:3)
task$head()
```

`$rbind()` and `$cbind()` allow to add extra rows and columns to a task.(The original data is not changed).

```{r}
task$cbind(data.table(foo = letters[1:3]))
task$head()
```

### 2.2.5 Plotting Tasks

The `mlr3viz` package provides plotting facilities for many implemented in `mlr3`.
all plots are returned as ggplot2 objects which can be easily customized.

- `mlr3viz::autoplot.TaskClassif`

```{r}
library(mlr3viz)

# get the pima indians task
task = tsk('pima')

# subset task to only use the 3 first features
task$select(head(task$feature_names, 3))

# default plot: class frequencies
autoplot(task)
```

Customizing the ggplot which is made from `autoplot()`

```{r}
p <- autoplot(task)
p + theme_minimal() + 
  labs(x = 'Diabetes', y = 'Count') +
  scale_fill_brewer(palette = 'Set1')
```

```{r, message=FALSE, warning=FALSE}
# pairs plot (requires package GGally)
autoplot(task, type = 'pairs')
```

```{r, message=FALSE, warning=FALSE}
# duo plot requires package GGally)
autoplot(task, type = 'duo') 
```


For regression task: `mlr3viz::autoplot.TaskRegr`

```{r}
task = tsk('mtcars')
task$select(head(task$feature_names, 3))
autoplot(task)
```

```{r, message=FALSE, warning=FALSE}
autoplot(task, type = 'pairs')
```



## 2.3 Learners

- mlr_learners_classif.featureless
- mlr_learners_classif.rpart
- mlr_learners_regr.featureless
- mlr_learners_regr.rpart

Some of the most popular learners are connected via the mlr3learners package:

 - (penalized) linear and logistic regeression
 - k-Nearest Neighbor regeression and classification
 - Linear and Quadratic Discriminant Analysis
 - Naive Bayes
 - Support-Vector machines
 - Gradient Boosting
 - Random Forest(Regression and Classification)
 - Kirging
 
mlr3learners organization

### 2.3.1 Predefined Learners

```{r}
mlr_learners
```

Each learner has the following information:

- feature_types: the type of features the learner can deal with
- packages: the packages required to train a model with this learner and make predictions
- properties: additional properties and capabilities. 
   * missing, importance
- predict_types: possible prediction types
   * label('response'), probabilities('prob')
   
```{r}
learner = mlr_learners$get('classif.rpart')
learner
```

The field param_set stores a description of the hyperparameters the learner has, their ranges, defaults and current values

```{r}
learner$param_set
```


```{r}
learner$param_set$values = list(cp = .01, xval = 0)
learner
```

Note that this operation just overwrite all previously set parameters. If you just want to add or update hyperparameters, you can use `mlr3misc::insert_named()`

```{r}
learner$param_set$values = mlr3misc::insert_named(
  learner$param_set$values,
  list(cp = .02, minsplit = 2)
)

learner
```

This updates `cp` to 0.02, sets `minsplit` to 2 and keeps the previously set parameter `xval`

```{r}
lrn('classif.rpart', id = 'rp', cp = .001)
```

## 2.4 Train and Predict

the iris dataset, rpart learner 

Training a learner means fitting a model to a given dataset.


### 2.4.1 Creating Task and Learner Objects

1. The classification task:
```{r}
task = tsk('sonar')
```

2. A learner for the classification tree.
```{r}
learner = lrn('classif.rpart')
learner
```

or 

```{r}
learner = mlr_learners$get('classif.rpart')
learner
```


### 2.4.2 Setting up the train/test splits of the data

set the train_set rowids and test_set rowids

```{r}
train_set = sample(task$nrow, .8*task$nrow)
test_set = setdiff(seq_len(task$nrow), train_set)
```


### 2.4.3 Training the learner

```{r}
learner$model
```

```{r}
learner$train(task, row_ids = train_set)
```

```{r}
learner$model
```

### 2.4.4 Predicting

```{r}
prediction = learner$predict(task, row_ids = test_set)
prediction
```

- The `$predict()` method of the Learner returns a Prediction object. More precisely, a LearnerClassif returns a PredictionClassif object

- A prediction object holds the row ids of the test data, the respective ture label of the target column and the respective predictions. The simplest way to extract this information is by converting the Prediction object to a `data.table`()

```{r}
prediction %>% as.data.table()
```

For classification, you can also extract the confusion matrix

```{r}
prediction$confusion
```


### 2.4.5 Changing the Predict Type

To switch to predicting these probabilities, the `predict_type` field of a `LearnerClassif` must be changed from `response` to `prob` before training.

```{r}
learner$predict_type = 'prob'

# re-fit the model
learner$train(task, row_ids = train_set)


# rebuild prediction object
prediction = learner$predict(task, row_ids = test_set)
```

The prediction object now contains probabilities for all class labels:

```{r}
prediction %>% as.data.table()
```


Directly access the predicted labels:

```{r}
prediction$response
```

Directly access the matrix of probabilities:
```{r}
prediction$prob %>% head
```

Analogously to predicting probabilities, many regression learners support the extraction of standard error estimates by setting the predict type to 'se'

### 2.4.6. Plotting Predictions

`mlr3viz` provides a `autoplot()` method for Prediction objects. autoplot.PredictionClassif(), autoplot.PredictionClassif()

```{r}
library("mlr3viz")

task = tsk("sonar")
task
```


```{r}
learner = lrn('classif.rpart', predict_type = 'prob')
learner
```

```{r}
train_set = sample(task$nrow, .8*task$nrow)
test_set = setdiff(seq_len(task$nrow), train_set)
```

```{r}
learner$train(task, row_ids = train_set)
```

```{r}
prediction = learner$predict(task, row_ids = test_set)
```

```{r}
autoplot(prediction)
```



```{r}
autoplot(prediction, type = 'roc')
```

```{r}
library("mlr3viz")
library("mlr3learners")


local({ # we do this locally to not overwrite the objects from previous chunks
  task = tsk("mtcars")
  train_set = sample(task$nrow, .8*task$nrow)
  test_set = setdiff(seq_len(task$nrow), train_set)
  learner = lrn("regr.lm")
  learner$train(task, row_ids = train_set)
  prediction = learner$predict(task, row_ids = test_set)
  autoplot(prediction)
})
```


### 2.4.7 Performance assessment

```{r}
mlr_measures
```


```{r}
measure = msr('classif.acc')
prediction$score(measure)
```

## 2.5 Resampling

mlr3 entails 6 predefined resampling strategies.
 - Cross-validation
 - Leave-one-out cross validation
 - Repeated cross validation
 - Out of bag bootstrap and other variants(e.g. b632)
 - Monte-Carlo cross-validation
 - Hold out
 
### 2.5.1 Settings

```{r}
task = tsk('iris')
learner = lrn('classif.rpart')
```

```{r}
as.data.table(mlr_resamplings)
```

 mlr3spatiotemporal
 
```{r}
mlr_resamplings$get('holdout')
```
 
```{r}
rsmp('holdout')
```
 
```{r}
resampling = rsmp('holdout')
resampling
```

Note that the $is_instantiated field is set to FALSE. This means we did not actually apply the strategy on a dataset yet. 

By default we get a .66/.33 split of the data. There are two ways in which the ratio can be changed:

1. Overwriting the slot in `$param_set$values` using a named list
```{r}
resampling$param_set$values  = list(ratio = .8)
resampling
```

2. Specifying the resampling parameters directly during construction:
```{r}
rsmp('holdout', ratio = .8)
```


### 2.5.2 Instantiation

```{r}
resampling = rsmp('cv', folds = 3L)
resampling$instantiate(task)
```


```{r}
resampling$iters
```

```{r}
resampling$test_set(1) %>% str()
```


```{r}
resampling$test_set(2) %>% str()
```

```{r}
resampling$test_set(3) %>% str()
```


### 2.5.3 Execution

With a `Task`, a `Learner` and a `Resampling` object we can call `resample()`, which fits the learner to the task at hand according to the given resampling strategy. This in turn creates a `ResampleResul`t object.

```{r}
task = tsk('pima')
learner = lrn('classif.rpart', maxdepth = 3, predict_type = 'prob')
resampling = rsmp("cv", folds = 3L)
```

```{r}
rr = resample(task, learner, resampling, store_models = TRUE)
```

```{r}
rr
```

Calculate the average performance across all resampling iterations:

```{r}
rr$aggregate(msr('classif.ce'))
```

Extract the performance for the individual resampling iterations

```{r}
rr$score(msr('classif.ce'))
```


```{r}
rr$warnings
```

```{r}
rr$errors
```


Extract and inspect the resampling splits:
```{r}
rr$resampling
```


```{r}
rr$resampling$iters
```

```{r}
rr$resampling$train_set(1) %>% str
```


Retrieve the learner of a specific iteration and inspect it:
```{r}
lrn = rr$learners[[1]]
lrn$model
```

Extract the predictions:

```{r}
rr$prediction() # all predictions merged into a single Prediction
```

```{r}
rr$predictions()[[1]] # prediction of first resampling iteration
```


### 2.5.4 Custom resampling

A manual resampling instance can be created using the "custom" template.

```{r}
resampling = rsmp('custom')
resampling$instantiate(task,
                       train = list(c(1:10, 51:60, 101:110)),
                       test = list(c(11:20, 61:70, 111:120))
                       )
resampling$iters
```


```{r}
resampling$train_set(1)
```

```{r}
resampling$test_set(1)
```

2.5.5 Plotting Resample Results

```{r}
autoplot(rr)
```


```{r}
autoplot(rr, type = 'roc')
```


## 2.6 Benchmarking

Comparing the performance of different learners on multiple tasks and/or different resampling schemes is a common task. This operation is usually referred to as “benchmarking” in the field of machine-learning.

### 2.6.1 Design Creation

By “design” we essentially mean the matrix of settings you want to execute. 

A “design” consists of unique combinations of Task, Learner and Resampling triplets.

```{r}
design = benchmark_grid(
  tasks = tsk('iris'),
  learners = list(lrn('classif.rpart'), lrn('classif.featureless')),
  resampling = rsmp('holdout')
)

design
```


```{r}
bmr = benchmark(design)
```

```{r}
bmr
```

 If you create the design manually, even if the same task is used multiple times, the train/test splits will be different for each row of the design if you do not manually instantiate the resampling before creating the design.

```{r}
# get some example tasks
tasks = lapply(c('german_credit', 'sonar'), tsk)
```

```{r}
# get some learners and for all learners
# * predict prob
# * predict also on the training set

# install.packages('kknn')

learners = c('classif.featureless', 'classif.rpart', 'classif.ranger', 'classif.kknn')
learners = lapply(learners, 
                  lrn,
                  predict_type = 'prob', 
                  predict_sets = c('train', 'test')
                  )
```


```{r}
# compare via 3-fold cross validation
resamplings = rsmp('cv', folds = 3)
```

```{r}
# Create a BenchmarkDesign object
design = benchmark_grid(tasks, learners, resamplings)

design
```

### 2.6.2 Execution and Aggregation of Results

```{r}
# execute the benchmark
bmr = benchmark(design)
```

```{r}
# measures:

# * auc on training
# * auc on test

measures = list(
  msr('classif.auc', id = 'auc_train', predict_sets = 'train'),
  msr('classif.auc', id = 'auc_test')
)
```

```{r}
bmr$aggregate(measures)
```

Subsequently, we can aggregate the results further. For example, we might be interested which learner performed best over all tasks simultaneously. Simply aggregating the performances with the mean is usually not statistically sound. Instead, we calculate the rank statistic for each learner grouped by task.

```{r}
tab = bmr$aggregate(measures)
tab
```

Then the calculated ranks grouped by learner are aggregated with data.table. Since the AUC needs to be maximized, we multiply with  `−1` so that the best learner gets a rank of `1`.

```{r}
# group by levels of task_id, return colunms:
# - learner_id
# - rank of col `-auc_train` 
# - rank of col `-auc_test`

# used by data.table() grammer.
ranks = tab[, .(learner_id, rank_train = rank(-auc_train), rank_test = rank(-auc_test)), by = task_id]
```

```{r}
ranks
```

```{r}
# group by levels of learner_id, return columns
# -mean rank of col `rank_train`
# -mean rank of col `rank_test`

ranks = ranks[, .(mrank_train = mean(rank_train), mrank_test = mean(rank_test)), 
              by = learner_id]

ranks
```

```{r}
ranks[order(mrank_test)]
```

### 2.6.3 Plotting Benchmarking Results

```{r}
autoplot(bmr) + coord_flip()
```


We can also plot ROC curves. To do so, we first need to filter the BenchmarkResult to only contain a single Task:

```{r}
autoplot(bmr$clone()$filter(task_id = "german_credit"), type = "roc")
```

```{r}
autoplot(bmr$clone()$filter(task_id = "sonar"), type = "roc")
```

### 2.6.4 Extracting ResampleResults

A BenchmarkResult object is essentially a collection of multiple ResampleResult objects. As these are stored in a column of the aggregated data.table(), we can easily extract them:

```{r}
tab = bmr$aggregate(measures)
rr = tab[task_id == 'sonar' & learner_id == 'classif.ranger']$resample_result[[1]]
rr
```

```{r}
measure = msr('classif.auc')
rr$aggregate(measure)
```

```{r}
# get the iteration with worst AUC
perf = rr$score(measure)
i = which.min(perf$classif.auc)
```

```{r}
rr$learners[[i]]
```


```{r}
rr$resampling$train_set(i) %>% head()
```


### Converting and Merging ResampleResults

It is also possible to cast a single ResampleResult to a BenchmarkResult using the converter `as_benchmark_result()`.

```{r}
task = tsk('iris')
resampling = rsmp('holdout')$instantiate(task)
```

```{r}
rr1 = resample(task, lrn("classif.rpart"), resampling)
rr2 = resample(task, lrn("classif.featureless"), resampling)
```

```{r}
# Cast both ResampleResult to BenchmarkResults
bmr1 = as_benchmark_result(rr1)
bmr2 = as_benchmark_result(rr2)
```

```{r}
# Merge 2nd BMR into the first BMR
bmr1$combine(bmr2)
```

```{r}
bmr1
```

## 2.7 Bianary classification

For such binary target variables, you can specify the positive class within the classification task object during task creation. If not explicitly set during construction, the positive class defaults to the first level of the target variable.

```{r}
# during construction
data("Sonar", package = "mlbench")
task = TaskClassif$new(id = "Sonar", Sonar, target = "Class", positive = "R")
```


```{r}
# switch positive class to level "M"
task$positive = "M"
```

### 2.7.1 ROC Curve and Thresholds

```{r}
learner = lrn("classif.rpart", predict_type = "prob")

resampling = rsmp('holdout')
resampling$instantiate(task)

train_idx = resampling$train_set(1)
test_idx = resampling$test_set(1)

rr = resample(task, learner, resampling, store_models = T)

learner$train(task, row_ids = train_idx)

pred = learner$predict(task, row_ids = test_idx)

C = pred$confusion

C
```















